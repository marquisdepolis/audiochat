{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Exception in thread Thread-4 (listen_and_transcribe):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/bl/nkj7dq8s6m16hf4zmr47vkfr0000gn/T/ipykernel_75118/2567227741.py\", line 101, in listen_and_transcribe\n",
      "  File \"/var/folders/bl/nkj7dq8s6m16hf4zmr47vkfr0000gn/T/ipykernel_75118/2567227741.py\", line 53, in record_after_keyphrase\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/pyaudio/__init__.py\", line 570, in read\n",
      "    return pa.read_stream(self._stream, num_frames,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno -9981] Input overflowed\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import whisper\n",
    "import webrtcvad\n",
    "import threading\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Constants\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK_DURATION_MS = 30\n",
    "VAD_PADDING_DURATION_MS = 2000\n",
    "CHUNK_SIZE = int(RATE * CHUNK_DURATION_MS / 1000)\n",
    "VAD_PADDING_CHUNKS = int(VAD_PADDING_DURATION_MS / CHUNK_DURATION_MS)\n",
    "KEY_PHRASE = \"Hey Grimoire\"\n",
    "MAX_RECORD_DURATION_MS = 10000\n",
    "MAX_RECORD_CHUNKS = int(MAX_RECORD_DURATION_MS / CHUNK_DURATION_MS)\n",
    "\n",
    "# Initialize VAD and PyAudio\n",
    "vad = webrtcvad.Vad(1)\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Initialize global variables for conversation history\n",
    "conversation_history = []\n",
    "\n",
    "def ask_llm(question):\n",
    "    \"\"\"\n",
    "    Simulated function to interact with an LLM and manage conversation history.\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history.append(f\"User: {question}\")\n",
    "    response = \"This is a simulated LLM response.\"\n",
    "    conversation_history.append(f\"GPT: {response}\")\n",
    "\n",
    "    # Truncate history if it exceeds a certain size\n",
    "    max_history = 5  # for example, keep the last 5 exchanges\n",
    "    if len(conversation_history) > max_history * 2:\n",
    "        conversation_history = conversation_history[-max_history * 2:]\n",
    "\n",
    "    return response\n",
    "\n",
    "def record_after_keyphrase():\n",
    "    \"\"\"\n",
    "    Record audio after the key phrase is detected.\n",
    "    \"\"\"\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK_SIZE)\n",
    "    frames = []\n",
    "    num_silent_chunks = 0\n",
    "    num_recorded_chunks = 0\n",
    "    keyphrase_detected = False\n",
    "\n",
    "    while num_recorded_chunks < MAX_RECORD_CHUNKS:\n",
    "        try:\n",
    "            chunk = stream.read(CHUNK_SIZE)\n",
    "            is_speech = vad.is_speech(chunk, RATE)\n",
    "            # ... [rest of your loop] ...\n",
    "        except IOError as e:\n",
    "            if e.errno == pyaudio.paInputOverflowed:\n",
    "                print(\"Input overflowed, skipping this chunk.\")\n",
    "                chunk = '\\x00' * CHUNK_SIZE  # Dummy chunk of silent audio\n",
    "\n",
    "        if not keyphrase_detected:\n",
    "            if is_speech:\n",
    "                audio_data = b''.join(frames)\n",
    "                temp_file = \"temp.wav\"\n",
    "                wf = wave.open(temp_file, 'wb')\n",
    "                wf.setnchannels(CHANNELS)\n",
    "                wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "                wf.setframerate(RATE)\n",
    "                wf.writeframes(audio_data)\n",
    "                wf.close()\n",
    "\n",
    "                model = whisper.load_model(\"base\")\n",
    "                result = model.transcribe(temp_file)\n",
    "                if KEY_PHRASE.lower() in result[\"text\"].lower():\n",
    "                    keyphrase_detected = True\n",
    "                    frames = []  # Start fresh frames\n",
    "            else:\n",
    "                frames = []  # Clear frames if no speech detected\n",
    "        else:\n",
    "            num_recorded_chunks += 1\n",
    "            if not is_speech:\n",
    "                num_silent_chunks += 1\n",
    "                if num_silent_chunks >= VAD_PADDING_CHUNKS:\n",
    "                    break\n",
    "            else:\n",
    "                num_silent_chunks = 0\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    return b''.join(frames)\n",
    "\n",
    "def transcribe_audio(audio_data):\n",
    "    \"\"\"\n",
    "    Transcribe audio data using Whisper.\n",
    "    \"\"\"\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_data)\n",
    "    return result[\"text\"]\n",
    "\n",
    "def listen_and_transcribe():\n",
    "    \"\"\"\n",
    "    Continuously listen for the key phrase and transcribe the speech.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        audio_data = record_after_keyphrase()\n",
    "        question = transcribe_audio(audio_data)\n",
    "        if question:\n",
    "            print(\"Question:\", question)\n",
    "            response = ask_llm(question)\n",
    "            print(\"Response:\", response)\n",
    "\n",
    "# Run the process in a separate thread\n",
    "thread = threading.Thread(target=listen_and_transcribe)\n",
    "thread.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
